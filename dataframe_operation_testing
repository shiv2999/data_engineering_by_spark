import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object DataFrameOperationsTest {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("DataFrameOperationsTest")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    // Sample data
    val data = Seq(
      (1, "Alice", 23, "A"),
      (2, "Bob", 29, "B"),
      (3, "Charlie", 31, "C"),
      (4, "David", 22, "B"),
      (5, "Eva", 25, "A"),
      (6, "Frank", null.asInstanceOf[Integer], "C")
    )

    val df = data.toDF("id", "name", "age", "grade")

    // Show original data
    println("Original DataFrame:")
    df.show()

    // Select and Rename Columns
    df.select("name", "age").show()
    df.withColumnRenamed("grade", "class_grade").show()

    // Filter Rows
    df.filter($"age" > 25).show()
    df.filter($"grade" === "A").show()

    // Add New Column
    df.withColumn("age_plus_10", $"age" + 10).show()

    // Conditional Column
    df.withColumn("status", when($"age" >= 25, "Senior").otherwise("Junior")).show()

    // Drop Column
    df.drop("grade").show()

    // GroupBy and Aggregations
    df.groupBy("grade")
      .agg(
        count("*").alias("count"),
        avg("age").alias("avg_age"),
        max("age").alias("max_age")
      ).show()

    // Sort
    df.sort($"age").show()
    df.sort($"age".desc).show()

    // Join
    val df2 = Seq((1, "Math"), (2, "Science"), (3, "History")).toDF("id", "subject")
    df.join(df2, Seq("id"), "inner").show()

    // Union
    val df3 = Seq((7, "Grace", 28, "B")).toDF("id", "name", "age", "grade")
    df.union(df3).show()

    // Drop Duplicates
    val dfWithDupes = df.union(df)
    dfWithDupes.dropDuplicates().show()

    // Null Handling
    df.filter($"age".isNull).show()
    df.na.fill(Map("age" -> 0)).show()

    // Describe
    df.describe().show()

    // Cache and Persist
    df.cache()
    df.persist()

    spark.stop()
  }
}
